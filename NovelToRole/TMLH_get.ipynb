{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.获取小说资源\n",
    "    自行下载epub文件，在此转换为txt格式并合并为一个文件,已有txt格式则忽略这步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ebooklib import epub\n",
    "# from bs4 import BeautifulSoup\n",
    "# import os\n",
    "\n",
    "# def extract_text_from_epub(file_path):\n",
    "#     book = epub.read_epub(file_path)\n",
    "#     text_content = []\n",
    "#     for item in book.items:\n",
    "#         if item.media_type == 'application/xhtml+xml':\n",
    "#             soup = BeautifulSoup(item.content,'html.parser')\n",
    "#             text_content.append(soup.get_text())\n",
    "#     return \"\\n\".join(text_content)\n",
    "\n",
    "# def merge_epubs_to_txt(epub_files, output_txt_file):\n",
    "#     with open(output_txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for file_name in os.listdir(epub_files):\n",
    "#             if file_name.endswith('.epub'):\n",
    "#                 file_path = os.path.join(epub_files,file_name)\n",
    "#                 print(f\"Porcess {file_name}...\")\n",
    "#                 text = extract_text_from_epub(file_path)\n",
    "#                 f.write(text + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# # 使用示例\n",
    "# epub_files = \"./novel/败犬1-6-epub\"\n",
    "# output_txt_file = \"TooManyLosingHeroines.txt\"   \n",
    "# merge_epubs_to_txt(epub_files, output_txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.小说切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择你要处理的小说，接下来的保存路径都将使用小说名\n",
    "novel_name = \"TooManyLosingHeroines\"\n",
    "input_txt_name = f\"./novel/{novel_name}.txt\"\n",
    "save_folder = f\"./extract/{novel_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意，文件夹 ./extract/TooManyLosingHeroines 已经存在\n",
      "49\n",
      "0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "败\n",
      "1 \n",
      "    专业青梅竹\n",
      "2 \n",
      "    为你献上注\n",
      "3 \n",
      "    未战先输　\n",
      "4 \n",
      "    当你凝望败\n",
      "5 『哥哥真的很努力了喔\n",
      "6 蝉声如雨。\n",
      "隔天的第\n",
      "7 隔天，合宿第一天。放\n",
      "8 合宿的隔天。星期一早\n",
      "9 八奈见杏菜想暗示\n",
      "I\n",
      "10 朝云千早令人混乱\n",
      "11 \n",
      "    唯独没被甩\n",
      "12 烧盐柠檬开了口\n",
      "尾声\n",
      "13 「我回来了……」\n",
      "我\n",
      "14 与朝云千早约定好提供\n",
      "15 这一天的上午。太阳已\n",
      "16 隔天夜里。我在精文馆\n",
      "17 志喜屋梦子多方关照\n",
      "\n",
      "18 姬宫华恋隆重登场\n",
      "I\n",
      "19 \n",
      "    要说再见还\n",
      "20 \n",
      "    来讨论责任\n",
      "21 西校舍的角落，文艺社\n",
      "22 文艺部的顾问老师决定\n",
      "23 石蕗祭当天早晨，我正\n",
      "24 石蕗祭结束后过了三天\n",
      "25 \n",
      "    别看我这样\n",
      "26 \n",
      "    只要多一点\n",
      "27 \n",
      "    为我的心取\n",
      "28 \n",
      "    十六岁的序\n",
      "29 放学后的社办中，摆在\n",
      "30 星期五课程结束后的班\n",
      "31 12月21日，星期一\n",
      "32 12月25日早晨。阳\n",
      "33 \n",
      "    道高一尺，\n",
      "34 \n",
      "    以浑身解数\n",
      "35 \n",
      "    欲擒主将，\n",
      "36 \n",
      "    放手的觉悟\n",
      "37 期末考第四天结束，就\n",
      "38 周末过去，星期一到学\n",
      "39 隔天的放学后。我来到\n",
      "40 石蕗高中参观会当天。\n",
      "41 \n",
      "    海风的告白\n",
      "42 \n",
      "    流泪愈多，\n",
      "43 \n",
      "    道别的季节\n",
      "44 \n",
      "    烧盐柠檬这\n",
      "45 石蕗高中文艺社的社办\n",
      "46 周末结束后的星期一总\n",
      "47 万里无云的星期六午后\n",
      "48 八奈见发出敌对宣言后\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "else:\n",
    "    print('注意，文件夹',save_folder,'已经存在')\n",
    "\n",
    "raw_text = open(input_txt_name, encoding='utf-8').read()\n",
    "\n",
    "# 将文本按章节切分\n",
    "chapters = []\n",
    "chapter_contents = []\n",
    "\n",
    "for line in raw_text.split('\\n'):\n",
    "    Flag = False\n",
    "    if line.strip().startswith(\"～第\"):\n",
    "        # 遇到章节标题,将之前章节内容添加到结果列表\n",
    "\n",
    "        head = line.strip()\n",
    "        # print(head)\n",
    "        head = head[:min(10,len(head))]\n",
    "        if head.find('败',1)>0:\n",
    "            # print(head)\n",
    "            Flag = True\n",
    "\n",
    "    if Flag:\n",
    "        if chapter_contents:\n",
    "            chapters.append('\\n'.join(chapter_contents))\n",
    "            chapter_contents = []\n",
    "        # 记录当前章节标题\n",
    "        # chapters.append(line)\n",
    "    else:\n",
    "        # 累积章节内容\n",
    "        chapter_contents.append(line)\n",
    "\n",
    "# 添加最后一个章节内容\n",
    "if chapter_contents:\n",
    "    chapters.append('\\n'.join(chapter_contents))\n",
    "# 检查结果\n",
    "print(len(chapters))\n",
    "for i,ch in enumerate(chapters):\n",
    "    print(i,ch[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title  定义divide函数，用来切分超长文本\n",
    "def divide_str(s, sep=[\"\\n\", \".\", \"。\"]):\n",
    "    mid_len = len(s) // 2  # 中心点位置\n",
    "    best_sep_pos = len(s) + 1  # 最接近中心点的分隔符位置\n",
    "    best_sep = None  # 最接近中心点的分隔符\n",
    "    for curr_sep in sep:\n",
    "        sep_pos = s.rfind(curr_sep, 0, mid_len)  # 从中心点往左找分隔符\n",
    "        if sep_pos > 0 and abs(sep_pos - mid_len) < abs(best_sep_pos - mid_len):\n",
    "            best_sep_pos = sep_pos\n",
    "            best_sep = curr_sep\n",
    "    if not best_sep:  # 没有找到分隔符\n",
    "        return s, \"\"\n",
    "    return s[: best_sep_pos + 1], s[best_sep_pos + 1 :]\n",
    "\n",
    "\n",
    "def strong_divide(s):\n",
    "    left, right = divide_str(s)\n",
    "\n",
    "    if right != \"\":\n",
    "        return left, right\n",
    "\n",
    "    whole_sep = [\n",
    "        \"\\n\",\n",
    "        \".\",\n",
    "        \"，\",\n",
    "        \"、\",\n",
    "        \";\",\n",
    "        \",\",\n",
    "        \"；\",\n",
    "        \"：\",\n",
    "        \"！\",\n",
    "        \"？\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"”\",\n",
    "        \"“\",\n",
    "        \"’\",\n",
    "        \"‘\",\n",
    "        \"[\",\n",
    "        \"]\",\n",
    "        \"{\",\n",
    "        \"}\",\n",
    "        \"<\",\n",
    "        \">\",\n",
    "        \"/\",\n",
    "        \"\"\"''', '|', '-', '=', '+', '*', '%', \\\n",
    "               '$', \"\"\"  #''', '@', '&', '^', '_', '`', '~',\\\n",
    "        \"·\",\n",
    "        \"…\",\n",
    "    ]\n",
    "    left, right = divide_str(s, sep=whole_sep)\n",
    "\n",
    "    if right != \"\":\n",
    "        return left, right\n",
    "\n",
    "    mid_len = len(s) // 2\n",
    "    return s[:mid_len], s[mid_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n",
      "就在这时，突然走进教室的男人笔直地走向我的座位。「呐，温水。听说你加入文艺社了？」\n",
      "「呃……」\n",
      "向我搭话的是D班的绫野光希。我们是同一所国中出身，算是朋友──其实只是以前上同一所补习班，他偶尔会找我搭话，这种程度的交情而已。\n",
      "顺带一提，我和他只是上同一间补习班，成绩是那家伙好过我许多。毕竟他还戴着眼镜。\n",
      "「呃，是啊。算是这样没错。」\n",
      "「我听老师说那边有安部公房的全集，下次可以去借书吗？」\n",
      "哦～原来还有这种书喔。我只注意过轻小说的藏书种类而已。\n",
      "「这个，应该没关系吧。我去找学长姐拜托看看。」\n",
      "「不好意思，谢啦。」\n",
      "绫野面露爽朗的笑容，拍打我的肩膀后，随即准备离开此处。\n",
      "就在这时，小麦色泽的身影冲进视野中。\n",
      "现身的是烧盐柠檬。那双晒黑的手臂压在桌面上。\n",
      "「光希！先等一下。」\n",
      "烧盐的身子朝着绫野前倾，8×4与汗水彼此混合的气味顿时飘荡在空气中。\n",
      "\n",
      "\n",
      "\n",
      "译注：制汗喷雾。\n",
      "\n",
      "\n",
      "……太近了。而且很碍事。\n",
      "「我今天田径队不用练习喔，要不要一起去吃点东西？」\n",
      "「抱歉，今天要上补习班。」\n",
      "绫野双手合十，摆出「抱歉」的小动作。\n",
      "「咦～不是才一年级吗？老是念书会变成笨蛋喔。」\n",
      "「你倒是该多念点书。不然会被留级喔。」\n",
      "……这两个家伙，居然在我的座位旁边开始打情骂俏了。\n",
      "「光希同学，再不快点出发，会赶不上补习班喔。」\n",
      "教室的入口处，一位身材苗条的女学生探出头来。\n",
      "咦？我记得这个女生，就是在补习班常常和绫野出双入对的女生。人长得可爱、成绩又很好，在补习班相当知名，原来我们同校喔。\n",
      "「啊啊，千早。我马上来。那我先走啦，柠檬。」\n",
      "「欸……嗯，掰掰……」\n",
      "毫不掩饰心中失落，烧盐消沉地挥了挥手。\n",
      "感觉好像很麻烦，我想早点离开，不过因为被烧盐挡住，我没办法拿书包。\n",
      "「那、那个……烧盐同学……那个……书包……」\n",
      "「呐，原来温水和光希是朋友喔？你们不同班吧？」\n",
      "烧盐疑惑地眨着眼，两排纤长的睫毛也随之颤动，她直盯着我的脸看。\n",
      "──田径队的短跑主将，烧盐柠檬。在班上是相当引人注目的女学生。\n",
      "剪短的头发包覆着小巧的脸蛋，从制服伸出的四肢苗条而精实，被晒成了好看的小麦色。我一瞬间看呆，佯装平静地开口说道：\n",
      "「呃……也算不上朋友。只是之前上同一间补习班，偶尔会讲几句话。」\n",
      "烧盐突然间眼神绽放光芒。\n",
      "「是同一间补习班喔！那你认识刚才那个女生吗！？」\n",
      "烧盐情绪激动，脸庞直逼向我眼前，这下我也难掩惊慌。\n",
      "「她喔，我记得是朝云同学吧？和绫野同样是升学班，成绩应该也差不多优秀。」\n",
      "「这、这样啊。光希那家伙，是不是真的比较喜欢聪明的女生啊……」\n",
      "语毕，她神色不安地凝视着两人消失的方向。\n",
      "嗯？烧盐这家伙，该不会……\n",
      "「他们两个在同一个升学班，那时常常在一起就是了。我觉得他们只是朋友喔。」\n",
      "「就是说嘛！他们只是普通朋友！」\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title 以1500 token为限，切分chunk，输出总chunk数量\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_token_len = 1500\n",
    "chunk_text = []\n",
    "\n",
    "\n",
    "for chapter in chapters:\n",
    "\n",
    "    split_text = chapter.split(\"\\n\")\n",
    "\n",
    "    curr_len = 0\n",
    "    curr_chunk = \"\"\n",
    "\n",
    "    tmp = []\n",
    "\n",
    "    for line in split_text:\n",
    "        line_len = len(enc.encode(line))\n",
    "\n",
    "        if line_len <= max_token_len - 5:\n",
    "            tmp.append(line)\n",
    "        else:\n",
    "            # print('divide line with length = ', line_len)\n",
    "            path = [line]\n",
    "            tmp_res = []\n",
    "\n",
    "            while path:\n",
    "                my_str = path.pop()\n",
    "                left, right = strong_divide(my_str)\n",
    "\n",
    "                len_left = len(enc.encode(left))\n",
    "                len_right = len(enc.encode(right))\n",
    "\n",
    "                if len_left > max_token_len - 15:\n",
    "                    path.append(left)\n",
    "                else:\n",
    "                    tmp_res.append(left)\n",
    "\n",
    "                if len_right > max_token_len - 15:\n",
    "                    path.append(right)\n",
    "                else:\n",
    "                    tmp_res.append(right)\n",
    "\n",
    "            for line in tmp_res:\n",
    "                tmp.append(line)\n",
    "\n",
    "    split_text = tmp\n",
    "\n",
    "    for line in split_text:\n",
    "        line_len = len(enc.encode(line))\n",
    "\n",
    "        if line_len > max_token_len:\n",
    "            print(\"warning line_len = \", line_len)\n",
    "\n",
    "        if curr_len + line_len <= max_token_len:\n",
    "            curr_chunk += line\n",
    "            curr_chunk += \"\\n\"\n",
    "            curr_len += line_len\n",
    "            curr_len += 1\n",
    "        else:\n",
    "            chunk_text.append(curr_chunk)\n",
    "            curr_chunk = line\n",
    "            curr_len = line_len\n",
    "\n",
    "    if curr_chunk:\n",
    "        chunk_text.append(curr_chunk)\n",
    "\n",
    "    # break\n",
    "\n",
    "print(len(chunk_text))\n",
    "print(chunk_text[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.重组小说\n",
    "    将小说重组为人物对话和剧情摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\boyu\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: kor in c:\\users\\boyu\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.8 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.12)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.25)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.145)\n",
      "Requirement already satisfied: numpy<2,>=1.26.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain_community) (8.2.3)\n",
      "Requirement already satisfied: pandas<3,>=1.5.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from kor) (2.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from kor) (2.10.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.8->langchain_community) (0.3.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.21->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from pandas<3,>=1.5.3->kor) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from pandas<3,>=1.5.3->kor) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.5.3->kor) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=2->kor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=2->kor) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain_community) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.5.3->kor) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\boyu\\anaconda3\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\boyu\\anaconda3\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (0.1.145)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain-openai) (1.58.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (0.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.67.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\boyu\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\boyu\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai<2.0.0,>=1.55.3->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community kor\n",
    "!pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object, Text, Number\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# 设置你自己的LLM key与model\n",
    "key = os.getenv(\"OPENAI_API_KEY\", \"your_api_key\")\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 仅仅抽取dialogue的部分\n",
    "\n",
    "# @title 抽取role,dialogue,action\n",
    "\n",
    "schema = Object(\n",
    "    id=\"script\",\n",
    "    description='''Adapted from the novel into script,\n",
    "    The main character,“温水”, refers to himself as “我” ''',\n",
    "    attributes=[\n",
    "        Text(\n",
    "            id=\"role\",\n",
    "            description=\"The character who is speaking, use context to predict the name of the role.\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"dialogue\",\n",
    "            description=\"The dialogue spoken by the characters in the sentence\",\n",
    "        ),\n",
    "        Text(\n",
    "            id =\"action\",\n",
    "            descrtption = '''The actions performed by the characters in the text, A high-level summary of a character's behavior. action equals \"对话\" or \"独白\" or \"动作\", \n",
    "            equals \"对话\" if it's sentence in [], equals \"独白\" elif it's a psychological monologue not in [], equals \"动作\" else\n",
    "            ''',\n",
    "        ),\n",
    "    ],\n",
    "    examples=[\n",
    "        (\n",
    "            \"\"\"``八奈见不知何时来到我身旁，用手肘轻轻地顶我。\n",
    "「喔，烧盐同学好像喜欢绫野的样子。」\n",
    "「是喔。真是意外的组合呢。」\n",
    "「我和他们国中时同校，不过他们好像从国小就常常在一起。」\n",
    "「所以说，就和青梅竹马差不多啰。」\n",
    "青梅竹马……在八奈见眼中是这样吗？\n",
    "对着无法理解的我，八奈见无奈地耸了耸肩。\n",
    "「听我说，温水，女生可以分成两大类。如果不是青梅竹马，就是狐狸精。」\n",
    "原来如此，真豪爽的分类法。八奈见用严厉的表情看着我。``\"\"\",\n",
    "            [\n",
    "                {\"role\":\"八奈见\",\"dialogue\":\"八奈见不知何时来到我身旁，用手肘轻轻地顶我\",\"action\":\"动作\",},\n",
    "                {\"role\": \"八奈见\", \"dialogue\": \"喔，烧盐同学好像喜欢绫野的样子\",\"action\":\"对话\",},\n",
    "                {\"role\": \"温水\", \"dialogue\": \"是喔。真是意外的组合呢。\",\"action\":\"对话\",},\n",
    "                {\n",
    "                    \"role\": \"温水\",\n",
    "                    \"dialogue\": \"我和他们国中时同校，不过他们好像从国小就常常在一起\",\n",
    "                    \"action\":\"对话\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"温水\",\n",
    "                    \"dialogue\": \"青梅竹马……在八奈见眼中是这样吗？\",\n",
    "                    \"action\":\"独白\",\n",
    "                },\n",
    "                {\"role\": \"八奈见\", \"dialogue\": \"所以说，就和青梅竹马差不多啰\",\"action\":\"对话\",},\n",
    "                {\n",
    "                    \"role\": \"八奈见\",\n",
    "                    \"dialogue\": \"听我说，温水，女生可以分成两大类。如果不是青梅竹马，就是狐狸精\",\n",
    "                    \"action\":\"对话\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"温水\",\n",
    "                    \"dialogue\": \"原来如此，真豪爽的分类法\",\n",
    "                    \"action\":\"独白\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    many=True,\n",
    ")\n",
    "chain = create_extraction_chain(llm, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'script': [{'role': '志喜屋', 'dialogue': '别紧张……天爱星……很好搞定……', 'action': '志喜屋学姐不知为何用双手指头圈成爱心形状'}, {'role': '温水', 'dialogue': '就算是这样，要我去攻陷马剃同学，真的办不到啦。', 'action': '对话'}, {'role': '八奈见', 'dialogue': '温水想太多了啦。就只是和马剃同学打好关系，拜托她把同人志还给我们，这样而已嘛。对吧？学姐。', 'action': '对话'}, {'role': '温水', 'dialogue': '是这样的话，不管怎么想，还是八奈见同学去做比较容易吧？', 'action': '对话'}, {'role': '八奈见', 'dialogue': '那女生有点可怕，我不要。', 'action': '对话'}, {'role': '温水', 'dialogue': '学姐，我真的不觉得有办法和马剃同学打好关系耶。', 'action': '对话'}, {'role': '志喜屋', 'dialogue': '天爱星……只要温柔对待……很简单……', 'action': '对话'}, {'role': '温水', 'dialogue': '呃，可是──', 'action': '对话'}, {'role': '志喜屋', 'dialogue': '抓住把柄……卖人情……用罪恶感……堵住去路。', 'action': '志喜屋学姐低语，将小小的黑色棋子摆到盘面上'}, {'role': '志喜屋', 'dialogue': '我会……帮忙喔？', 'action': '对话'}, {'role': '温水', 'dialogue': '哎，算是啦。', 'action': '对话'}, {'role': '八奈见', 'dialogue': '既然学姐都说愿意帮忙了，你就做好觉悟吧。', 'action': '对话'}, {'role': '温水', 'dialogue': '但是说到底，我很不擅长应付女生啊。只要一面对面就会忘记怎么说话。', 'action': '对话'}, {'role': '八奈见', 'dialogue': '温水，你要不要偶尔想起来我是女的？', 'action': '八奈见把空的蛋糕盘还给我，双手合十对我说谢谢招待'}, {'role': '八奈见', 'dialogue': '再说你就是胡思乱想，才会觉得好像是亏心事。温水和妹妹相处得很好吧？就把马剃同学当作妹妹，温柔对待她就可以了吧？', 'action': '对话'}, {'role': '温水', 'dialogue': '请让我先确认一件事。在我看来，马剃同学对文艺社怀有敌意。这次的私物检查也一样，文艺社毫无疑问被她盯上了。', 'action': '对话'}, {'role': '志喜屋', 'dialogue': '没关系……我也觉得……天爱星……太过头了。', 'action': '志喜屋学姐摇晃般点头'}, {'role': '温水', 'dialogue': '我明白了，那就麻烦学姐帮忙了。当然，我会尽可能稳妥地处理这件事的。', 'action': '对话'}, {'role': '志喜屋', 'dialogue': '嗯……我也……只是帮忙……', 'action': '对话'}, {'role': '温水', 'dialogue': '……累坏了。', 'action': '回家后，我没有力气爬上二楼，在客厅的沙发上沉重地坐下'}]}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_chunk_id = 300\n",
    "response = chain.invoke({\"text\":f\"``{chunk_text[test_chunk_id]}\"})\n",
    "print(response[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Summarize the key points of the following text in a concise way, using bullet points.\n",
    "\"\"\"\n",
    "\n",
    "q_example = \"\"\"###\n",
    "Text:\n",
    "洪七公、周伯通、郭靖、黄蓉四人乘了小船，向西驶往陆地。郭靖坐在船尾扳桨，黄蓉不住向周伯通详问骑鲨游海之事，周伯通兴起，当场就要设法捕捉鲨鱼，与黄蓉大玩一场。\n",
    "郭靖见师父脸色不对，问道：“你老人家觉得怎样”洪七公不答，气喘连连，声息粗重。他被欧阳锋以“透骨打穴法”点中之后，穴道虽已解开，内伤却又加深了一层。黄蓉喂他服了几颗九花玉露丸，痛楚稍减，气喘仍是甚急。\n",
    "老顽童不顾别人死活，仍是嚷着要下海捉鱼，黄蓉却已知不妥，向他连使眼色，要他安安静静的，别吵得洪七公心烦。周伯通并不理会，只闹个不休。黄蓉皱眉道：“你要捉鲨鱼，又没饵引得鱼来，吵些甚么”\n",
    "\n",
    "Summarize in BULLET POINTS form:\n",
    "\"\"\"\n",
    "\n",
    "a_example = \"\"\"\n",
    "- 洪七公等四人乘船西行,洪七公因受内伤加重而气喘不止\n",
    "- 周伯通要捉鲨鱼玩,被黄蓉阻止以免掀翻小船\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- 主角被志喜屋学姐鼓励去与马剃同学打好关系以取回同人志\n",
      "- 主角对与女生打交道感到不安，尤其是马剃同学\n",
      "- 志喜屋学姐认为马剃同学容易对付，并愿意帮忙\n",
      "- 主角担心马剃同学对文艺社有敌意，需谨慎处理\n",
      "- 八奈见提醒主角不要误解任务为勾引\n",
      "- 主角感到疲惫，回家后在沙发上休息\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=q_example),\n",
    "    AIMessage(content=a_example),\n",
    "]\n",
    "\n",
    "\n",
    "new_input = f\"\"\"###\n",
    "Text:\n",
    "{chunk_text[test_chunk_id]}\n",
    "\n",
    "Summarize in BULLET POINTS form:\"\"\"\n",
    "\n",
    "messages.append(HumanMessage(content=new_input))\n",
    "\n",
    "response = llm(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [3:00:28<00:00, 18.61s/it]    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 最贵的一步，这里只处理几个chunk作为测试\n",
    "for i in tqdm(range(len(chunk_text))):\n",
    "    save_name = os.path.join(save_folder, f\"{i}_dialogue.txt\")\n",
    "\n",
    "    if not os.path.exists(save_name) or os.path.getsize(save_name) < 5:\n",
    "        if os.path.exists(save_name):\n",
    "            print(\"re-generate dialogue id = \", i)\n",
    "        query_text = f\"``{chunk_text[i]}``\"\n",
    "        # dialogue_response = chain.run( query_text )[\"data\"]\n",
    "        dialogue_response = chain.invoke({\"text\": f\"``{chunk_text[i]}``\"})[\"data\"]\n",
    "        with open(save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            if \"script\" not in dialogue_response:\n",
    "                print('Error: response does not contain key \"script\"')\n",
    "            else:\n",
    "                for chat in dialogue_response[\"script\"]:\n",
    "                    json_str = json.dumps(chat, ensure_ascii=False)\n",
    "                    f.write(json_str + \"\\n\")\n",
    "\n",
    "    save_name_sum = os.path.join(save_folder, f\"{i}_sum.txt\")\n",
    "\n",
    "    if not os.path.exists(save_name_sum) or os.path.getsize(save_name_sum) < 5:\n",
    "        if os.path.exists(save_name_sum):\n",
    "            print(\"re-summarize id = \", i)\n",
    "        # dealing with summarize\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=q_example),\n",
    "            AIMessage(content=a_example),\n",
    "        ]\n",
    "\n",
    "        new_input = f\"###\\nText:\\n{chunk_text[ i ]}\\nSummarize in BULLET POINTS form:\"\n",
    "\n",
    "        messages.append(HumanMessage(content=new_input))\n",
    "\n",
    "        summarize_response = llm(messages).content\n",
    "\n",
    "        with open(save_name_sum, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summarize_response)\n",
    "\n",
    "    raw_text_save_name = os.path.join(save_folder, f\"{i}_raw.txt\")\n",
    "\n",
    "    if (\n",
    "        not os.path.exists(raw_text_save_name)\n",
    "        or os.path.getsize(raw_text_save_name) < 5\n",
    "    ):\n",
    "        with open(raw_text_save_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(chunk_text[i])\n",
    "    # # for test\n",
    "    # if i > 3:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将得到的raw,dialogue,sum组合为规格的json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = f\"./reorganized/{novel_name}\"\n",
    "# chunk所在文件夹，请以_raw结尾\n",
    "folder_path = f\"./extract/{novel_name}\"\n",
    "# 故事名字，默认为_raw之前的名字\n",
    "story_name_en = novel_name\n",
    "\n",
    "save_jsonl_path = f\"{save_folder_path}/reorganized_{story_name_en}.jsonl\"\n",
    "save_txt_path = f\"{save_folder_path}/reorganized_{story_name_en}.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n"
     ]
    }
   ],
   "source": [
    "chunk_text = [\"\"] * (\n",
    "    sum([1 for file_name in os.listdir(folder_path) if file_name.endswith(\"_sum.txt\")])\n",
    ")\n",
    "\n",
    "# 遍历文件夹中的文件\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\"_raw.txt\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # 提取文件名中的 i\n",
    "        i = int(file_name.split(\"_\")[0])\n",
    "\n",
    "        # 打开文件并读取内容\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # 将文件内容添加到列表中，按 i 的顺序插入到正确位置\n",
    "        chunk_text[i] = text\n",
    "\n",
    "print(len(chunk_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': '烧盐柠檬', 'dialogue': '为你献上注定的败北', 'if_scene': False}, {'role': 'scene', 'dialogue': '提到“注定的败北”和“烧盐柠檬”', 'if_scene': True}, {'role': '老师', 'dialogue': '那种郁闷是什么，让老师来教·教·你', 'if_scene': False}, {'role': 'scene', 'dialogue': '提及一种郁闷感', 'if_scene': True}, {'role': 'scene', 'dialogue': '暗示老师将教授相关内容', 'if_scene': True}]\n",
      "['为你献上注定的败北', '提到“注定的败北”和“烧盐柠檬”', '那种郁闷是什么，让老师来教·教·你', '提及一种郁闷感', '暗示老师将教授相关内容']\n",
      "['提到“注定的败北”和“烧盐柠檬”', '提及一种郁闷感', '暗示老师将教授相关内容']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id = 2\n",
    "dialoge_file = os.path.join(folder_path, f\"{id}_dialogue.txt\")\n",
    "summarzie_file = os.path.join(folder_path, f\"{id}_sum.txt\")\n",
    "raw_text = chunk_text[id]\n",
    "chunk_sum = []\n",
    "unique_chunk_sum = []\n",
    "# 给定summarzie_file = os.path.join(save_folder, f\"{id}_sum.txt\")\n",
    "\n",
    "# 先检查这个文件是否存在\n",
    "\n",
    "# 然后使用utf-8编码打开，检查每一行，如果strip后，行首是'-'，则把后面的字符串append到一个list chunk_sum中\n",
    "\n",
    "# 请用python为我实现\n",
    "if os.path.exists(summarzie_file):\n",
    "    with open(summarzie_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"-\"):\n",
    "                chunk_sum.append(line.strip()[1:].strip())\n",
    "if os.path.exists(dialoge_file):\n",
    "    with open(dialoge_file, encoding=\"utf-8\") as f:\n",
    "        dialogues = []\n",
    "        for line in f:\n",
    "            dialogue = json.loads(line)\n",
    "            dialogues.append(dialogue)\n",
    "\n",
    "unique_dialogue = []\n",
    "for item in dialogues:\n",
    "    if item not in unique_dialogue:\n",
    "        unique_dialogue.append(item)\n",
    "dia_texts = [data[\"dialogue\"] for data in unique_dialogue]\n",
    "for item in chunk_sum:\n",
    "    if item not in unique_chunk_sum:\n",
    "        unique_chunk_sum.append(item)\n",
    "\n",
    "chunk_sum = unique_chunk_sum\n",
    "dialogues = unique_dialogue\n",
    "print(dialogues)\n",
    "print(dia_texts)\n",
    "print(chunk_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 给定长文本raw_text。\n",
    "\n",
    "# 使用换行符\\n或者。 来对这个字符串进行切割，忽略掉strip之后是空的子字符串\n",
    "\n",
    "# 将每一段话的起点位置存储在一个list of int , starts中\n",
    "\n",
    "# 将每一段话的结束位置存储在一个list of int , ends中\n",
    "\n",
    "# 并且将每一个子字符串的存储在一个list of str, lines中\n",
    "def divide_raw2lines(raw_text):\n",
    "    previous_str = \"\"\n",
    "    starts = []\n",
    "    ends = []\n",
    "    lines = []\n",
    "    for i in range(len(raw_text)):\n",
    "        previous_str += raw_text[i]\n",
    "        if raw_text[i] in (\"\\n\", \"。\"):\n",
    "            strip_str = previous_str.strip(' \"“”\\r\\n')\n",
    "            if len(strip_str) > 0:\n",
    "                lines.append(strip_str)\n",
    "                starts.append(i - len(strip_str))\n",
    "                ends.append(i)\n",
    "            previous_str = \"\"\n",
    "        else:\n",
    "            pass\n",
    "    return lines, starts, ends\n",
    "\n",
    "\n",
    "lines, starts, ends = divide_raw2lines(raw_text)\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已知if '\\u4e00' <= char <= '\\u9fa5': 可以判断一个char是否是中文字\n",
    "\n",
    "# 我希望实现一个函数，这个函数的输入是两个list of string, 长度为M的query 和 长度为N的datas\n",
    "\n",
    "# 输出是一个M*N的numpy float数组 recalls\n",
    "\n",
    "# 先计算freqs[m][n] 表示query的第m句中的每一个中文字，是否在datas[n]中是否出现，如果出现，则freqs[m][n]加一\n",
    "\n",
    "# 然后计算recalls[m][n]是freqs[m][n]除掉 query[m]中所有中文字的个数\n",
    "\n",
    "\n",
    "def compute_char_recall(query, datas):\n",
    "    M = len(query)\n",
    "    N = len(datas)\n",
    "\n",
    "    freqs = np.zeros((M, N), dtype=int)\n",
    "\n",
    "    for m in range(M):\n",
    "        q_chars = set()\n",
    "        for char in query[m]:\n",
    "            if \"\\u4e00\" <= char <= \"\\u9fa5\":\n",
    "                q_chars.add(char)\n",
    "\n",
    "        for n in range(N):\n",
    "            for char in q_chars:\n",
    "                if char in datas[n]:\n",
    "                    freqs[m][n] += 1\n",
    "\n",
    "    query_chars_count = [\n",
    "        len(set(char for char in sent if \"\\u4e00\" <= char <= \"\\u9fa5\"))\n",
    "        for sent in query\n",
    "    ]\n",
    "\n",
    "    recalls = freqs / np.array(query_chars_count)[:, None]\n",
    "\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0227272727272727 [1, 2, 2]\n",
      "\n",
      "###\n",
      "提到“注定的败北”和“烧盐柠檬”\n",
      "\n",
      "\n",
      "为你献上注定的败北　烧盐柠檬\n",
      "\n",
      "###\n",
      "提及一种郁闷感\n",
      "\n",
      "\n",
      "Intermission\n",
      "\n",
      "###\n",
      "暗示老师将教授相关内容\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import copy\n",
    "import numpy as np \n",
    "def summary2line(chunk_sum, lines):\n",
    "\n",
    "    s = compute_char_recall(chunk_sum, lines)\n",
    "\n",
    "    color_map = {}\n",
    "    ans_Q = {}\n",
    "\n",
    "    ans_div = {}\n",
    "\n",
    "    flags = {}\n",
    "\n",
    "    M = len(chunk_sum)\n",
    "    N = len(lines)\n",
    "\n",
    "    for n in range(0, N):\n",
    "        if n == 0:\n",
    "            ans_Q[(0, 0)] = s[0, 0]\n",
    "            ans_div[(0, 0)] = []\n",
    "        else:\n",
    "            ans_Q[(0, n)] = ans_Q[(0, n - 1)] + s[0, n]\n",
    "            ans_div[(0, n)] = []\n",
    "\n",
    "    for m in range(1, M):\n",
    "        ans_Q[(m, m)] = ans_Q[(m - 1, m - 1)] + s[m, m]\n",
    "        ans_div[(m, m)] = ans_div[(m - 1, m - 1)].copy()\n",
    "        ans_div[(m, m)].append(m)\n",
    "\n",
    "    def find_Q(m, n):\n",
    "        # print(m,n)\n",
    "\n",
    "        if m < 0 or n < 0:\n",
    "            print(\"error out bound\", m, \" \", n)\n",
    "            return 0, []\n",
    "\n",
    "        if (m, n) in ans_Q.keys():\n",
    "            return ans_Q[(m, n)], ans_div[(m, n)]\n",
    "\n",
    "        if (m, n) in color_map.keys():\n",
    "            print(\"error repeated quest \", m, \" \", n)\n",
    "            return 0, []\n",
    "        else:\n",
    "            color_map[(m, n)] = 1\n",
    "\n",
    "        current_div = []\n",
    "\n",
    "        left, left_div = find_Q(m, n - 1)\n",
    "        right, right_div = find_Q(m - 1, n - 1)\n",
    "\n",
    "        if left > right:\n",
    "            ans = left + s[m][n]\n",
    "            flags[(m, n)] = False\n",
    "            current_div = left_div\n",
    "\n",
    "        else:\n",
    "            ans = right + s[m][n]\n",
    "            flags[(m, n)] = True\n",
    "            current_div = right_div.copy()\n",
    "            current_div.append(n - 1)\n",
    "\n",
    "        # ans = max(  , ) + s[m][n]\n",
    "\n",
    "        ans_Q[(m, n)] = ans\n",
    "        ans_div[(m, n)] = current_div.copy()\n",
    "\n",
    "        return ans, current_div\n",
    "\n",
    "    # print(find_Q(0,5))\n",
    "    # print(find_Q(M-1,N-1))\n",
    "\n",
    "    score, divs = find_Q(M - 1, N - 1)\n",
    "    divs.append(N - 1)\n",
    "\n",
    "    return score, divs\n",
    "\n",
    "\n",
    "score, divs = summary2line(chunk_sum, lines)\n",
    "print(score, divs)\n",
    "\n",
    "last = 0\n",
    "\n",
    "# divs.append(N-1)\n",
    "\n",
    "for i, div in enumerate(divs):\n",
    "    print(\"\\n###\")\n",
    "    print(chunk_sum[i])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for j in range(last, div):\n",
    "        print(lines[j])\n",
    "        last = div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 2, 2]\n",
      "\n",
      "###\n",
      "为你献上注定的败北\n",
      "为你献上注定的败北　烧盐柠檬\n",
      "\n",
      "\n",
      "\n",
      "###\n",
      "提到“注定的败北”和“烧盐柠檬”\n",
      "为你献上注定的败北　烧盐柠檬\n",
      "\n",
      "\n",
      "\n",
      "###\n",
      "那种郁闷是什么，让老师来教·教·你\n",
      "那种郁闷是什么，让老师来教·教·你\n",
      "\n",
      "\n",
      "\n",
      "###\n",
      "提及一种郁闷感\n",
      "那种郁闷是什么，让老师来教·教·你\n",
      "\n",
      "\n",
      "\n",
      "###\n",
      "暗示老师将教授相关内容\n",
      "那种郁闷是什么，让老师来教·教·你\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dialogue2line(dia_texts, lines):\n",
    "    s_dialogue = compute_char_recall(dia_texts, lines)\n",
    "\n",
    "    M, N = s_dialogue.shape\n",
    "    if M == 0 or N == 0:\n",
    "        return []\n",
    "    dp = np.zeros((M, N))\n",
    "    dp[0] = s_dialogue[0]\n",
    "    prev_indices = np.zeros((M, N), dtype=int)\n",
    "    for i in range(1, M):\n",
    "        for j in range(N):\n",
    "            max_prev_index = np.argmax(dp[i - 1])\n",
    "            dp[i][j] = dp[i - 1][max_prev_index] + s_dialogue[i][j]\n",
    "            prev_indices[i][j] = max_prev_index\n",
    "\n",
    "    max_end_index = np.argmax(dp[-1])\n",
    "    sequence = []\n",
    "    for i in range(M - 1, -1, -1):\n",
    "        sequence.append(max_end_index)\n",
    "        max_end_index = prev_indices[i][max_end_index]\n",
    "    sequence.reverse()\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "seq = dialogue2line(dia_texts, lines)\n",
    "print(seq)\n",
    "\n",
    "for i, id in enumerate(seq):\n",
    "    print(\"\\n###\")\n",
    "    print(dia_texts[i])\n",
    "    print(lines[id])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for j in range(last, div):\n",
    "        print(lines[j])\n",
    "        last = div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[1, 2, 2]\n",
      "['为你献上注定的败北', '提到“注定的败北”和“烧盐柠檬”', '那种郁闷是什么，让老师来教·教·你', '提及一种郁闷感', '暗示老师将教授相关内容']\n",
      "[0, 0, 2, 2, 2]\n",
      "[{'role': '烧盐柠檬', 'text': '为你献上注定的败北', 'if_scene': False}, {'role': 'scene', 'text': '提到“注定的败北”和“烧盐柠檬”', 'if_scene': False}, {'role': 'scene', 'text': '提到“注定的败北”和“烧盐柠檬”', 'if_scene': True}, {'role': '老师', 'text': '那种郁闷是什么，让老师来教·教·你', 'if_scene': False}, {'role': 'scene', 'text': '提及一种郁闷感', 'if_scene': True}, {'role': 'scene', 'text': '提及一种郁闷感', 'if_scene': False}, {'role': 'scene', 'text': '暗示老师将教授相关内容', 'if_scene': True}, {'role': 'scene', 'text': '暗示老师将教授相关内容', 'if_scene': False}]\n"
     ]
    }
   ],
   "source": [
    "print(len(chunk_sum))\n",
    "print(divs)\n",
    "print(dia_texts)\n",
    "print(seq)\n",
    "\n",
    "\n",
    "# string_indices = [10, 11]\n",
    "def jsonl_sorted(chunk_sum, divs, dia_texts, seq):\n",
    "\n",
    "    combined_data = []\n",
    "    combined_text = \"\"\n",
    "    for index in sorted(seq + divs):\n",
    "        # print(index)\n",
    "        if index in seq:\n",
    "\n",
    "            combined_data.append(\n",
    "                {\n",
    "                    \"role\": dialogues[seq.index(index)][\"role\"],\n",
    "                    \"text\": dialogues[seq.index(index)][\"dialogue\"],\n",
    "                    \"if_scene\": False,\n",
    "                }\n",
    "            )\n",
    "            combined_text = (\n",
    "                combined_text\n",
    "                + dialogues[seq.index(index)][\"role\"]\n",
    "                + \":\"\n",
    "                + dialogues[seq.index(index)][\"dialogue\"]\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            seq[seq.index(index)] = -1\n",
    "        if index in divs:\n",
    "            combined_data.append(\n",
    "                {\n",
    "                    \"role\": \"scene\",\n",
    "                    \"text\": chunk_sum[divs.index(index)],\n",
    "                    \"if_scene\": True,\n",
    "                }\n",
    "            )\n",
    "            combined_text = (\n",
    "                combined_text + \"scene\" + \":\" + chunk_sum[divs.index(index)] + \"\\n\"\n",
    "            )\n",
    "            divs[divs.index(index)] = -1\n",
    "\n",
    "    return combined_data, combined_text\n",
    "\n",
    "\n",
    "combined_data, combined_text = jsonl_sorted(\n",
    "    chunk_sum, divs.copy(), dia_texts, seq.copy()\n",
    ")\n",
    "print(combined_data)\n",
    "# 打开文件，以写入模式（\"w\"）打开\n",
    "with open(dialoge_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    # 遍历数据列表中的每个字典\n",
    "    for record in combined_data:\n",
    "        # 将字典转换为JSON格式的字符串\n",
    "        json_record = json.dumps(record, ensure_ascii=False)\n",
    "        # 将转换后的JSON字符串写入文件，并添加换行符\n",
    "        file.write(json_record + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"rm -rf {save_folder_path}\")\n",
    "os.makedirs(save_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/581 [00:00<?, ?item/s]C:\\Users\\boyu\\AppData\\Local\\Temp\\ipykernel_21548\\281538791.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = freqs / np.array(query_chars_count)[:, None]\n",
      "Processing:   2%|▏         | 14/581 [00:00<00:04, 137.28item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个chunk出错\n",
      "第4个chunk出错\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 199/581 [00:01<00:03, 122.48item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第188个chunk出错\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 314/581 [00:02<00:02, 132.84item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第291个chunk出错\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 410/581 [00:03<00:01, 126.78item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第390个chunk出错\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 581/581 [00:04<00:00, 121.24item/s]\n"
     ]
    }
   ],
   "source": [
    "final_jsonl = []\n",
    "final_txt = \"\"\n",
    "\n",
    "for i in tqdm(\n",
    "    range(1, len(chunk_text)), desc=\"Processing\", total=len(chunk_text) - 1, unit=\"item\"\n",
    "):\n",
    "\n",
    "    try:\n",
    "        # story_name_en = 'shediaoyingxiongzhuan'\n",
    "        raw_text = chunk_text[i]\n",
    "\n",
    "        import os\n",
    "\n",
    "        save_folder = f\"./extract/{story_name_en}\"\n",
    "\n",
    "        dialoge_file = os.path.join(save_folder, f\"{i}_dialogue.txt\")\n",
    "        summarzie_file = os.path.join(save_folder, f\"{i}_sum.txt\")\n",
    "\n",
    "        chunk_sum = []\n",
    "        unique_chunk_sum = []\n",
    "        if os.path.exists(summarzie_file):\n",
    "            with open(summarzie_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip().startswith(\"-\"):\n",
    "                        chunk_sum.append(line.strip()[1:].strip())\n",
    "        if os.path.exists(dialoge_file):\n",
    "            with open(dialoge_file, encoding=\"utf-8\") as f:\n",
    "                dialogues = []\n",
    "                for line in f:\n",
    "                    dialogue = json.loads(line)\n",
    "                    dialogues.append(dialogue)\n",
    "\n",
    "        unique_dialogue = []\n",
    "        for item in dialogues:\n",
    "            if item not in unique_dialogue:\n",
    "                unique_dialogue.append(item)\n",
    "        dia_texts = [data[\"dialogue\"] for data in unique_dialogue]\n",
    "        for item in chunk_sum:\n",
    "            if item not in unique_chunk_sum:\n",
    "                unique_chunk_sum.append(item)\n",
    "\n",
    "        chunk_sum = unique_chunk_sum\n",
    "        dialogues = unique_dialogue\n",
    "        lines, starts, ends = divide_raw2lines(raw_text)\n",
    "        # print(chunk_sum)\n",
    "        # print(lines)\n",
    "        score, divs = summary2line(chunk_sum, lines)  # summary匹配\n",
    "        seq = dialogue2line(dia_texts, lines)  # 对话匹配\n",
    "        combined_data, combined_text = jsonl_sorted(\n",
    "            chunk_sum, divs.copy(), dia_texts, seq.copy()\n",
    "        )\n",
    "        # 如果需要保存每个chunk的，在此处保存\n",
    "        final_jsonl.append(combined_data)\n",
    "        final_txt = final_txt + combined_text + \"\\n\"\n",
    "    except:\n",
    "        print(\"第\" + str(i) + \"个chunk出错\")\n",
    "        pass\n",
    "with open(save_jsonl_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    # 遍历数据列表中的每个字典\n",
    "    for record in final_jsonl:\n",
    "        # 将字典转换为JSON格式的字符串\n",
    "        json_record = json.dumps(record, ensure_ascii=False)\n",
    "        # 将转换后的JSON字符串写入文件，并添加换行符\n",
    "        file.write(json_record + \"\\n\")\n",
    "with open(save_txt_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.小说chatbot抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "\n",
    "# 支持跨越多少行寻找目标角色，也即控制段内行间距不超过该值\n",
    "max_find_lines = 10\n",
    "\n",
    "max_token_num = 500\n",
    "\n",
    "# target_role支持 空字符串(默认前三个)或者List of string 如果出错默认保存第一个\n",
    "target_role = [\"八奈见\", \"我\"]\n",
    "\n",
    "# 输入文件路径\n",
    "input_jsonl_name = (\n",
    "    f\"./reorganized/{novel_name}/reorganized_{novel_name}.jsonl\"\n",
    ")\n",
    "\n",
    "# 保存路径\n",
    "savepath = f\"./role/{novel_name}\"\n",
    "os.system(f\"rm -rf {savepath}\")\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "# zip_path = '/content/linghuchong_text.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17771\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# input_name = '/content/reorganized_xiaoaojianghu.jsonl'\n",
    "# 用utf8编码为我读取jsonl文件，并且把每一行解析成一个list of json, data\n",
    "# 请用python为我实现\n",
    "\n",
    "# # 输入文件路径\n",
    "# input_name = '/content/shediaoyingxiongzhuan.jsonl'\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "data_in_chunk = []\n",
    "with open(input_jsonl_name, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        data_in_chunk.append(json.loads(line))\n",
    "\n",
    "data = []\n",
    "\n",
    "for chunk in data_in_chunk:\n",
    "    for d in chunk:\n",
    "        data.append(d)\n",
    "\n",
    "print(len(data))\n",
    "for i, d in enumerate(data):\n",
    "    if d[\"role\"] == \"scene\":\n",
    "        first_scene_id = i\n",
    "        break\n",
    "print(first_scene_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_scene_chat_id(data, target_role_single):\n",
    "\n",
    "    chat_ids = []\n",
    "\n",
    "    # 先寻找所有出现角色的节点\n",
    "    for i, d in enumerate(data):\n",
    "        if d[\"role\"] == target_role_single:\n",
    "            chat_ids.append(i)\n",
    "\n",
    "    previous_scene_ids = []\n",
    "\n",
    "    # 对于每一个chat_ids，向前寻找scene的节点\n",
    "    for chat_id in chat_ids:\n",
    "        ans = first_scene_id\n",
    "        for j in range(chat_id, first_scene_id, -1):\n",
    "            if data[j][\"role\"] == \"scene\":\n",
    "                ans = j\n",
    "                break\n",
    "        previous_scene_ids.append(ans)\n",
    "    return chat_ids, previous_scene_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chats2chunks(chat_ids, previous_scene_ids):\n",
    "    chat_ids_in_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for chat_id in chat_ids:\n",
    "        if not current_chunk:\n",
    "            current_chunk.append(chat_id)\n",
    "            continue\n",
    "\n",
    "        if chat_id - current_chunk[-1] <= max_find_lines:\n",
    "            current_chunk.append(chat_id)\n",
    "        else:\n",
    "            chat_ids_in_chunk.append(current_chunk)\n",
    "            current_chunk = [chat_id]\n",
    "\n",
    "    if current_chunk:\n",
    "        chat_ids_in_chunk.append(current_chunk)\n",
    "\n",
    "    # print(chat_ids_in_chunk[0])\n",
    "\n",
    "    chat_id2previous_scene_id = {}\n",
    "\n",
    "    for previous, chat_id in zip(previous_scene_ids, chat_ids):\n",
    "        chat_id2previous_scene_id[chat_id] = previous\n",
    "        if previous > 0:\n",
    "            if data[previous - 1][\"role\"] != target_role:\n",
    "                chat_id2previous_scene_id[chat_id] -= 1\n",
    "    return chat_ids_in_chunk, chat_id2previous_scene_id\n",
    "\n",
    "\n",
    "# chat_ids的分块， chat_id对应的旁白id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(my_str):\n",
    "    return len(enc.encode(my_str))\n",
    "\n",
    "\n",
    "def data2str(data):\n",
    "    role = data[\"role\"]\n",
    "    if role in [\"旁白\", \"\", \"scene\", \"Scene\", \"narrator\", \"Narrator\"]:\n",
    "        return \"scene:\" + data[\"text\"]\n",
    "    else:\n",
    "        return role + \":「\" + data[\"text\"] + \"」\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2texts(data, chat_ids_in_chunk, chat_id2previous_scene_id):\n",
    "    line_token = [count_token(data2str(d)) for d in data]\n",
    "    from ast import Break\n",
    "\n",
    "    final_chunks = []\n",
    "\n",
    "    print_count = 0\n",
    "\n",
    "    appended_key = []\n",
    "\n",
    "    for chunk in chat_ids_in_chunk:\n",
    "        N = len(chunk)\n",
    "\n",
    "        current_i = 0\n",
    "\n",
    "        while current_i < N - 1:\n",
    "\n",
    "            consider_chat_id = chunk[current_i]\n",
    "\n",
    "            previous_scene_id = chat_id2previous_scene_id[consider_chat_id]\n",
    "\n",
    "            # 保底\n",
    "            withdraw_start = previous_scene_id\n",
    "            withdraw_end = consider_chat_id\n",
    "\n",
    "            current_count = sum(line_token[previous_scene_id : consider_chat_id + 1])\n",
    "            while current_count < max_token_num and current_i < N - 1:\n",
    "                consider_end = chunk[current_i + 1]\n",
    "                consider_count = sum(line_token[previous_scene_id : consider_end + 1])\n",
    "                if consider_count < max_token_num:\n",
    "                    current_count = consider_count\n",
    "                    withdraw_start = previous_scene_id\n",
    "                    withdraw_end = consider_end\n",
    "                    current_i += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # print_count += 1\n",
    "\n",
    "            # print(withdraw_start, end = ' ')\n",
    "            # if print_count % 5 == 0:\n",
    "            #     print()\n",
    "\n",
    "            if withdraw_end + 1 not in appended_key:\n",
    "                appended_key.append(withdraw_end + 1)\n",
    "                chunk_str = \"\"\n",
    "                for i in range(withdraw_start, withdraw_end + 1):\n",
    "                    chunk_str += data2str(data[i]) + \"\\n\"\n",
    "\n",
    "                final_chunks.append(chunk_str)\n",
    "\n",
    "            current_i += 1\n",
    "    return appended_key, final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def save_chunk2zip(savepath, save_title, final_chunks):\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    for i in range(0, len(final_chunks)):\n",
    "        my_str = final_chunks[i]\n",
    "        with open(savepath + f\"/text_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(my_str)\n",
    "    zip_path = \"./role/\" + save_title + \"_text.zip\"\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "        for filename in os.listdir(savepath):\n",
    "            zipf.write(savepath + \"/\" + filename)\n",
    "\n",
    "    print(\"Zipped folder saved to\", zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped folder saved to ./role/八奈见_text.zip\n",
      "Zipped folder saved to ./role/我_text.zip\n"
     ]
    }
   ],
   "source": [
    "for role_cur_name in target_role:\n",
    "    chat_ids, previous_scene_ids = output_scene_chat_id(data, role_cur_name)\n",
    "    chat_ids_in_chunk, chat_id2previous_scene_id = divide_chats2chunks(\n",
    "        chat_ids, previous_scene_ids\n",
    "    )\n",
    "    appended_key, final_chunks = id2texts(\n",
    "        data, chat_ids_in_chunk, chat_id2previous_scene_id\n",
    "    )\n",
    "    save_chunk2zip(\n",
    "        savepath + \"/\" + role_cur_name, role_cur_name, final_chunks\n",
    "    )  # 如果你想修改保存的zip名称，请修改本函数的第二个参数save_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.抽取embedding\n",
    "获得角色的jsonl资料后需要进行embedding构建支持RAG的知识库，这一步移到test文件里进行了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
